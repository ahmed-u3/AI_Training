{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"X0dJfdkeZgNy"},"outputs":[],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","from testCases_v4a import *\n","from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1vcDevhZgN5"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","\n","\n","    W1 = np.random.randn(n_h, n_x) * 0.01\n","    b1 = np.zeros(shape=(n_h, 1))\n","    W2 = np.random.randn(n_y, n_h) * 0.01\n","    b2 = np.zeros(shape=(n_y, 1))\n","\n","\n","    assert(W1.shape == (n_h, n_x))\n","    assert(b1.shape == (n_h, 1))\n","    assert(W2.shape == (n_y, n_h))\n","    assert(b2.shape == (n_y, 1))\n","\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WjUIkwwZgN6","outputId":"0fc905f1-d9d7-4b6a-909a-b62536d52709"},"outputs":[{"name":"stdout","output_type":"stream","text":["W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n"," [-0.01072969  0.00865408 -0.02301539]]\n","b1 = [[0.]\n"," [0.]]\n","W2 = [[ 0.01744812 -0.00761207]]\n","b2 = [[0.]]\n"]}],"source":["parameters = initialize_parameters(3,2,1)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vx9Oz_sBZgN8"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters_deep\n","\n","def initialize_parameters_deep(layer_dims):\n","\n","\n","    parameters = {}\n","    L = len(layer_dims)            # number of layers in the network\n","\n","    for l in range(1, L):\n","\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","\n","\n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNG0H6TyZgN9","outputId":"59263587-1d63-4b9e-9f13-4f47271094aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["W1 = [[ 0.00319039 -0.0024937   0.01462108 -0.02060141 -0.00322417]\n"," [-0.00384054  0.01133769 -0.01099891 -0.00172428 -0.00877858]\n"," [ 0.00042214  0.00582815 -0.01100619  0.01144724  0.00901591]\n"," [ 0.00502494  0.00900856 -0.00683728 -0.0012289  -0.00935769]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.00267888  0.00530355 -0.00691661 -0.00396754]\n"," [-0.00687173 -0.00845206 -0.00671246 -0.00012665]\n"," [-0.0111731   0.00234416  0.01659802  0.00742044]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n"]}],"source":["parameters = initialize_parameters_deep([5,4,3])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"josSR7UbZgN9"},"outputs":[],"source":["def linear_forward(A, W, b):\n","\n","\n","    Z = np.dot(W, A) + b\n","\n","\n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","\n","    return Z, cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROX6U7VxZgN-","outputId":"fd66b895-bfbf-4d8c-99ca-ecbb0f0d3e11"},"outputs":[{"name":"stdout","output_type":"stream","text":["Z = [[ 3.26295337 -1.23429987]]\n"]}],"source":["A, W, b = linear_forward_test_case()\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrqonCCFZgN_"},"outputs":[],"source":["# GRADED FUNCTION: linear_activation_forward\n","\n","def linear_activation_forward(A_prev, W, b, activation):\n","\n","\n","    if activation == \"sigmoid\":\n","\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","\n","\n","    elif activation == \"relu\":\n","\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","\n","\n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJeQ7D1HZgN_","outputId":"eebc1e06-34e9-4d66-fac5-7d628bdd4329"},"outputs":[{"name":"stdout","output_type":"stream","text":["With sigmoid: A = [[0.96890023 0.11013289]]\n","With ReLU: A = [[3.43896131 0.        ]]\n"]}],"source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiCDHOGHZgOA"},"outputs":[],"source":["# GRADED FUNCTION: L_model_forward\n","\n","def L_model_forward(X, parameters):\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2\n","\n","    for l in range(1, L):\n","        A_prev = A\n","\n","        A, cache = linear_activation_forward(A_prev,\n","                                             parameters['W' + str(l)],\n","                                             parameters['b' + str(l)],\n","                                             activation='relu')\n","        caches.append(cache)\n","\n","\n","\n","    AL, cache = linear_activation_forward(A,\n","                                          parameters['W' + str(L)],\n","                                          parameters['b' + str(L)],\n","                                          activation='sigmoid')\n","    caches.append(cache)\n","\n","\n","    assert(AL.shape == (1,X.shape[1]))\n","\n","    return AL, caches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W65leRcVZgOB","outputId":"3efb39f1-5f6a-4c77-d116-e91cdb3eba87"},"outputs":[{"name":"stdout","output_type":"stream","text":["AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n","Length of caches list = 3\n"]}],"source":["X, parameters = L_model_forward_test_case_2hidden()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emKo9GjIZgOB"},"outputs":[],"source":["\n","def compute_cost(AL, Y):\n","\n","    m = Y.shape[1]\n","\n","\n","    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n","\n","\n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","\n","    return cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"so4_ex1jZgOB","outputId":"ecbbc421-e74a-4e98-ce99-3fcb9f523974"},"outputs":[{"name":"stdout","output_type":"stream","text":["cost = 0.2797765635793422\n"]}],"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(AL, Y)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Th5CBbtVZgOC"},"outputs":[],"source":["# GRADED FUNCTION: linear_backward\n","\n","def linear_backward(dZ, cache):\n","\n","\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","\n","    dW = np.dot(dZ, cache[0].T) / m\n","    db = np.sum(dZ, axis=1, keepdims=True) / m\n","    dA_prev = np.dot(cache[1].T, dZ)\n","\n","\n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","\n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30e5aTSFZgOC","outputId":"8fcb914a-5db3-454d-ddbb-d2aac6f573eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db = [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n"]}],"source":["# Set up some test inputs\n","dZ, linear_cache = linear_backward_test_case()\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWIIFFPgZgOC"},"outputs":[],"source":["# GRADED FUNCTION: linear_activation_backward\n","\n","def linear_activation_backward(dA, cache, activation):\n","\n","\n","    linear_cache, activation_cache = cache\n","\n","    if activation == \"relu\":\n","\n","        dZ = relu_backward(dA, activation_cache)\n","\n","\n","    elif activation == \"sigmoid\":\n","\n","        dZ = sigmoid_backward(dA, activation_cache)\n","\n","\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHcJt-z5ZgOC","outputId":"da625e1d-1c7d-4f9d-f499-b4482280d360"},"outputs":[{"name":"stdout","output_type":"stream","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989 -0.        ]\n"," [ 0.37883606 -0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"]}],"source":["dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0iG1xYlZgOD"},"outputs":[],"source":["# GRADED FUNCTION: L_model_backward\n","\n","def L_model_backward(AL, Y, caches):\n","\n","\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","\n","    # Initializing the backpropagation\n","\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","\n","\n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","\n","    current_cache = caches[-1]\n","    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n","\n","\n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n","\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n","        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","\n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PETti2BZZgOD","outputId":"9d7b9487-695f-4e1c-d785-96dc0d03f519"},"outputs":[{"name":"stdout","output_type":"stream","text":["dW1 = [[-0.38142895 -0.05436378 -0.12122851 -0.09345065]\n"," [-0.36454443 -0.04886266 -0.11465667 -0.08859687]\n"," [-0.36758766 -0.04958047 -0.11573455 -0.08940829]]\n","db1 = [[0.13978379]\n"," [0.12259085]\n"," [0.12471635]]\n","dA1 = [[ 0.01969098 -0.12970306]\n"," [ 0.09890728 -0.22545732]\n"," [ 0.1304364  -0.31335009]\n"," [ 0.03215356  0.01532388]]\n"]}],"source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print_grads(grads)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87-ddtDOZgOD"},"outputs":[],"source":["# GRADED FUNCTION: update_parameters\n","\n","def update_parameters(parameters, grads, learning_rate):\n","\n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","\n","    for l in range(L):\n","        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoC_3RHMZgOE","outputId":"d76a5302-86fb-4b08-e8f9-78c5302ecdfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"]}],"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}